{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd2c36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[LdrGetDllHandle, LdrGetProcedureAddress, LdrG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            function  target\n",
       "0  [GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...       0\n",
       "1  [GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...       0\n",
       "2  [LdrGetDllHandle, LdrGetProcedureAddress, LdrG...       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정상 빈도수 x (성공)\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "txt = []\n",
    "bei = []\n",
    "\n",
    "path = \"C:\\\\test2//\" + '0//'\n",
    "files = os.listdir(path)\n",
    "files_list = len(files)\n",
    "type(files[0])\n",
    "    \n",
    "for file in files[:100]: \n",
    "    f = open(path + file, 'r')\n",
    "   \n",
    "    \n",
    "    while True:\n",
    "        line = f.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        txt.append(line)    \n",
    "\n",
    "    bei.append(txt)\n",
    "    txt = []\n",
    "f.close()\n",
    "\n",
    "\n",
    "# print(bei)\n",
    "bei\n",
    "\n",
    "#list를 DataFrame으로 \n",
    "bei_df = pd.DataFrame()\n",
    "bei_df['function'] = bei\n",
    "bei_df['target'] = [0]* files_list\n",
    "bei_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f171fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[NtAllocateVirtualMemory, NtFreeVirtualMemory,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[SetUnhandledExceptionFilter, NtAllocateVirtua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NtAllocateVirtualMemory, NtAllocateVirtualMem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            function  target\n",
       "0  [NtAllocateVirtualMemory, NtFreeVirtualMemory,...       1\n",
       "1  [SetUnhandledExceptionFilter, NtAllocateVirtua...       1\n",
       "2  [NtAllocateVirtualMemory, NtAllocateVirtualMem...       1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 악성 빈도수x\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "txt2 = []\n",
    "mal = []\n",
    "\n",
    "path = \"C:\\\\test2////\" + '1//'\n",
    "files2 = os.listdir(path)\n",
    "files_list2 = len(files2)\n",
    "type(files[0])\n",
    "    \n",
    "for file in files2[:100]: \n",
    "    f = open(path + file, 'r')\n",
    "   \n",
    "    \n",
    "    while True:\n",
    "        line2 = f.readline().strip()\n",
    "        if not line2:\n",
    "            break\n",
    "        txt2.append(line2)    \n",
    "\n",
    "    mal.append(txt2)\n",
    "    txt2 = []\n",
    "f.close()\n",
    "\n",
    "#list를 DataFrame으로 \n",
    "mal_df = pd.DataFrame()\n",
    "mal_df['function'] = mal\n",
    "mal_df['target'] = [1]* files_list2\n",
    "mal_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d4c0c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[LdrGetDllHandle, LdrGetProcedureAddress, LdrG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NtAllocateVirtualMemory, NtFreeVirtualMemory,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[SetUnhandledExceptionFilter, NtAllocateVirtua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[NtAllocateVirtualMemory, NtAllocateVirtualMem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            function  target\n",
       "0  [GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...       0\n",
       "1  [GetSystemTimeAsFileTime, LdrLoadDll, LdrGetPr...       0\n",
       "2  [LdrGetDllHandle, LdrGetProcedureAddress, LdrG...       0\n",
       "3  [NtAllocateVirtualMemory, NtFreeVirtualMemory,...       1\n",
       "4  [SetUnhandledExceptionFilter, NtAllocateVirtua...       1\n",
       "5  [NtAllocateVirtualMemory, NtAllocateVirtualMem...       1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상과 악성 데이터 프레임 합치기\n",
    "api_df = pd.concat([bei_df, mal_df], ignore_index=True)\n",
    "api_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f7008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp310-cp310-win_amd64.whl (438.0 MB)\n",
      "     -------------------------------------- 438.0/438.0 MB 5.3 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 KB ? eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 KB ? eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 KB ? eta 0:00:00\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp310-cp310-win_amd64.whl (903 kB)\n",
      "     ------------------------------------- 903.8/903.8 KB 11.5 MB/s eta 0:00:00\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 9.8 MB/s eta 0:00:00\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "     ------------------------------------- 462.5/462.5 KB 14.6 MB/s eta 0:00:00\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "     ---------------------------------------- 14.2/14.2 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.7/126.7 KB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\les85\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\les85\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (58.1.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\les85\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.8/97.8 KB 5.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 KB 9.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "     ------------------------------------- 224.9/224.9 KB 14.3 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.1/63.1 KB ? eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "     -------------------------------------- 156.7/156.7 KB 9.2 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ---------------------------------------- 155.3/155.3 KB ? eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.2/61.2 KB 3.4 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "     -------------------------------------- 149.2/149.2 KB 8.7 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "     -------------------------------------- 139.0/139.0 KB 8.0 MB/s eta 0:00:00\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 KB 4.5 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.5/151.5 KB 8.8 MB/s eta 0:00:00\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, certifi, wrapt, wheel, werkzeug, urllib3, typing-extensions, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, markdown, keras-preprocessing, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, cachetools, absl-py, requests, google-auth, astunparse, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Running setup.py install for termcolor: started\n",
      "  Running setup.py install for termcolor: finished with status 'done'\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.1 h5py-3.6.0 idna-3.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9c409b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ntallocatevirtualmemory': 1, 'ldrgetprocedureaddress': 2, 'ldrloaddll': 3, 'ntfreevirtualmemory': 4, 'getfiletype': 5, 'getsystemtimeasfiletime': 6, 'ntclose': 7, 'ldrgetdllhandle': 8, 'setunhandledexceptionfilter': 9, 'getsystemwindowsdirectoryw': 10, 'ntcreatefile': 11, 'ntreadfile': 12}\n"
     ]
    }
   ],
   "source": [
    "#딕셔너리화\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "A = api_df['function'].to_list()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(A)\n",
    "word_dic = tokenizer.word_index\n",
    "print(word_dic)\n",
    "\n",
    "# sequences = tokenizer.texts_to_sequences(A)\n",
    "# print(sequences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef7cf425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 3, 2, 3, 3, 2, 2], [6, 3, 2, 3, 3, 2, 2], [8, 2, 2, 1, 1, 4, 1, 4], [1, 4, 1, 1, 1, 1, 5, 5, 5], [9, 1, 1, 7, 3, 2, 2, 10], [1, 1, 11, 12, 7, 4, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(A)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7a5d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6, 3, 2, 3, 3, 2, 2]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 3, 2, 3, 3, 2, 2]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8, 2, 2, 1, 1, 4, 1, 4]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 1, 1, 1, 1, 5, 5, 5]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9, 1, 1, 7, 3, 2, 2, 10]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 1, 11, 12, 7, 4, 4, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      function  target\n",
       "0        [6, 3, 2, 3, 3, 2, 2]       0\n",
       "1        [6, 3, 2, 3, 3, 2, 2]       0\n",
       "2     [8, 2, 2, 1, 1, 4, 1, 4]       0\n",
       "3  [1, 4, 1, 1, 1, 1, 5, 5, 5]       1\n",
       "4    [9, 1, 1, 7, 3, 2, 2, 10]       1\n",
       "5   [1, 1, 11, 12, 7, 4, 4, 1]       1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_df['function'] = sequences\n",
    "api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37ce7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['006003 003002 002003 003003 003002 002002', '006003 003002 002003 003003 003002 002002', '008002 002002 002001 001001 001004 004001 001004', '001004 004001 001001 001001 001001 001005 005005 005005', '009001 001001 001007 007003 003002 002002 002010', '001001 001011 011012 012007 007004 004004 004001']\n"
     ]
    }
   ],
   "source": [
    "#n- gram(1)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = api_df['function'].to_list()\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for j in text:\n",
    "    two_gram = zip(j, j[1:])\n",
    "    for i in two_gram:\n",
    "        a.append(str(i[0]).zfill(3)+str(i[1]).zfill(3))\n",
    "        result = \" \".join(a)\n",
    "    b.append(result)\n",
    "    a = []\n",
    "    \n",
    "        \n",
    "        \n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "607093c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('001001', 0),\n",
       " ('001004', 1),\n",
       " ('001005', 2),\n",
       " ('001007', 3),\n",
       " ('001011', 4),\n",
       " ('002001', 5),\n",
       " ('002002', 6),\n",
       " ('002003', 7),\n",
       " ('002010', 8),\n",
       " ('003002', 9),\n",
       " ('003003', 10),\n",
       " ('004001', 11),\n",
       " ('004004', 12),\n",
       " ('005005', 13),\n",
       " ('006003', 14),\n",
       " ('007003', 15),\n",
       " ('007004', 16),\n",
       " ('008002', 17),\n",
       " ('009001', 18),\n",
       " ('011012', 19),\n",
       " ('012007', 20)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(b)\n",
    "tfidf_vectorizer.vocabulary_\n",
    "sorted(tfidf_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6004aec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28654804, 0.39607227, 0.        , 0.66878285,\n",
       "        0.39607227, 0.        , 0.        , 0.        , 0.39607227,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28654804, 0.39607227, 0.        , 0.66878285,\n",
       "        0.39607227, 0.        , 0.        , 0.        , 0.39607227,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.24480373, 0.67674492, 0.        , 0.        , 0.        ,\n",
       "        0.41264214, 0.24480373, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28567728, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.41264214, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.58300651, 0.26861431, 0.32757271, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22678265, 0.        , 0.65514541, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.26058239, 0.        , 0.        , 0.43923873, 0.        ,\n",
       "        0.        , 0.26058239, 0.        , 0.43923873, 0.30409043,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.43923873, 0.        , 0.        , 0.43923873, 0.        ,\n",
       "        0.        ],\n",
       "       [0.24567644, 0.        , 0.        , 0.        , 0.41411319,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2866957 , 0.41411319, 0.        , 0.        ,\n",
       "        0.        , 0.41411319, 0.        , 0.        , 0.41411319,\n",
       "        0.41411319]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf_wm(array 처리한 거)\n",
    "\n",
    "tfidf_wm = tfidf_vectorizer.fit_transform(b).toarray()\n",
    "tfidf_wm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c42a4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf_lr(array 처리안한 거 -> logistic regression 사용)\n",
    "tfidf_lr = tfidf_vectorizer.fit_transform(b)\n",
    "tfidf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc95163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 끝 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d2e32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(api_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bbb2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "y = np.array(targets)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "966ff7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy: {}\".format(lgs.score(X_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
